{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09f935f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\SHYNI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SHYNI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SHYNI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\SHYNI\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a91994e4405948dfb3799bebd0393e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/74 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset prepared and saved to 'processed_stories_dataset'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import GPT2Tokenizer\n",
    "from datasets import Dataset\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize NLTK tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove unnecessary characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.lower() not in stop_words]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Read the dataset\n",
    "def read_dataset(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Split the content by genre\n",
    "    pattern = r'<GENRE: (.*?)>\\n(.*?)\\nMoral: (.*?)\\n'\n",
    "    matches = re.findall(pattern, content, re.DOTALL)\n",
    "    \n",
    "    data = []\n",
    "    for match in matches:\n",
    "        genre, story, moral = match\n",
    "        data.append({\n",
    "            'genre': genre.strip(),\n",
    "            'story': preprocess_text(story.strip()),\n",
    "            'moral': preprocess_text(moral.strip())\n",
    "        })\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_data(data, tokenizer):\n",
    "    tokenized_data = {'input_ids': [], 'attention_mask': []}\n",
    "    for entry in data:\n",
    "        input_text = f\"<GENRE: {entry['genre']}>\\n{entry['story']}\\nMoral: {entry['moral']}\\n\"\n",
    "        tokenized_input = tokenizer(input_text, padding='max_length', truncation=True, max_length=512)\n",
    "        tokenized_data['input_ids'].append(tokenized_input['input_ids'])\n",
    "        tokenized_data['attention_mask'].append(tokenized_input['attention_mask'])\n",
    "    \n",
    "    return tokenized_data\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "file_path = 'stories.txt'\n",
    "data = read_dataset(file_path)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize the data\n",
    "tokenized_data = tokenize_data(data, tokenizer)\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "dataset = Dataset.from_dict(tokenized_data)\n",
    "\n",
    "# Save the processed dataset\n",
    "dataset.save_to_disk('processed_stories_dataset')\n",
    "\n",
    "print(\"Dataset prepared and saved to 'processed_stories_dataset'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25b69971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt2_with_pad\\\\tokenizer_config.json',\n",
       " 'gpt2_with_pad\\\\special_tokens_map.json',\n",
       " 'gpt2_with_pad\\\\vocab.json',\n",
       " 'gpt2_with_pad\\\\merges.txt',\n",
       " 'gpt2_with_pad\\\\added_tokens.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Add a pad token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Save the tokenizer with the new pad token\n",
    "tokenizer.save_pretrained('gpt2_with_pad')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff66870b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 26:07, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=27, training_loss=1.3223454510724102, metrics={'train_runtime': 1618.3994, 'train_samples_per_second': 0.137, 'train_steps_per_second': 0.017, 'total_flos': 56439078912000.0, 'train_loss': 1.3223454510724102, 'epoch': 2.918918918918919})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the modified tokenizer with padding token\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2_with_pad')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Load your preprocessed dataset\n",
    "dataset = Dataset.load_from_disk('processed_stories_dataset')\n",
    "\n",
    "# Convert dataset to PyTorch Dataset\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        item['input_ids'] = torch.tensor(item['input_ids'])\n",
    "        item['attention_mask'] = torch.tensor(item['attention_mask'])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "# Create custom dataset\n",
    "train_dataset = CustomDataset(dataset)\n",
    "\n",
    "# Fine-tuning parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0619627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./final_model\n"
     ]
    }
   ],
   "source": [
    "# Save the model and tokenizer\n",
    "model_save_path = './final_model'\n",
    "model.save_pretrained(model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1141c21a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nGPT2LMHeadModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15252\\2416127552.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_save_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./gpt2_with_pad\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Model and tokenizer loaded successfully\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\utils\\import_utils.py\u001b[0m in \u001b[0;36m__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1524\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"_from_config\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1525\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1526\u001b[1;33m         \u001b[0mrequires_backends\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backends\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\utils\\import_utils.py\u001b[0m in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1512\u001b[0m     \u001b[0mfailed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mavailable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchecks\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mavailable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfailed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1514\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfailed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: \nGPT2LMHeadModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(model_save_path)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2_with_pad\")\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0574798f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story 1:\n",
      "Once upon a time in a magical forest, the young girl named Kirito was able to find her way back home.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_story(prompt, max_length=150, num_return_sequences=1, temperature=0.7, top_k=50, top_p=0.9, repetition_penalty=1.2):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    stories = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    return stories\n",
    "\n",
    "\n",
    "prompt = \"Once upon a time in a magical forest\"\n",
    "stories = generate_story(prompt, temperature=0.7, top_k=50, top_p=0.9)\n",
    "\n",
    "for i, story in enumerate(stories):\n",
    "    print(f\"Story {i + 1}:\\n{story}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7280f3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Interactive storytelling function\n",
    "def interactive_storytelling():\n",
    "    print(\"Hello! What story do you want to hear today?\")\n",
    "    print(\"Type the number to choose:\")\n",
    "    print(\"1. Fairy Tale\")\n",
    "    print(\"2. Adventure\")\n",
    "    print(\"3. Fantasy\")\n",
    "    print(\"4. Sci-Fi\")\n",
    "    print(\"5. Mystery\")\n",
    "    print(\"6. Animal Tale\")\n",
    "    print(\"7. Fable\")\n",
    "    print(\"8. Mythology\")\n",
    "    print(\"9. Historical Fiction\")\n",
    "    print(\"10. Humor\")\n",
    "    print(\"11. Friendship\")\n",
    "    print(\"12. Superheroes\")\n",
    "    print(\"13. Sports\")\n",
    "    print(\"14. Holidays\")\n",
    "    print(\"15. Bedtime\")\n",
    "    \n",
    "    user_input = input(\"> \").strip()\n",
    "\n",
    "    genre_prompts = {\n",
    "        \"1\": (\"fairy tale\", \"a brave knight, a clever princess, or a talking animal\"),\n",
    "        \"2\": (\"adventure\", \"a thrilling journey, a treasure hunt, or a daring expedition\"),\n",
    "        \"3\": (\"fantasy\", \"elves, dragons, or wizards\"),\n",
    "        \"4\": (\"sci-fi\", \"distant planets, encounter aliens, or dive into futuristic technology\"),\n",
    "        \"5\": (\"mystery\", \"uncover a hidden treasure, solve a crime, or reveal a secret\"),\n",
    "        \"6\": (\"animal tale\", \"a wise owl, a brave lion, or a mischievous monkey\"),\n",
    "        \"7\": (\"fable\", \"wisdom, kindness, or perseverance\"),\n",
    "        \"8\": (\"mythology\", \"Greek, Norse, or Egyptian\"),\n",
    "        \"9\": (\"historical fiction\", \"ancient civilizations, medieval kingdoms, or the roaring twenties\"),\n",
    "        \"10\": (\"humor\", \"puns, slapstick comedy, or witty banter\"),\n",
    "        \"11\": (\"friendship\", \"loyalty, compassion, or teamwork\"),\n",
    "        \"12\": (\"superheroes\", \"flight, super strength, or invisibility\"),\n",
    "        \"13\": (\"sports\", \"soccer, basketball, or swimming\"),\n",
    "        \"14\": (\"holidays\", \"Halloween, Christmas, or New Year's Eve\"),\n",
    "        \"15\": (\"bedtime\", \"dreamlands, whispering forests, or starlit skies\")\n",
    "    }\n",
    "\n",
    "    if user_input in genre_prompts:\n",
    "        genre, options = genre_prompts[user_input]\n",
    "        print(f\"Wonderful! Do you want a story about {options}?\")\n",
    "        specific_choice = input(\"> \").lower()\n",
    "        prompt = f\"<GENRE: {genre}> Once upon a time, in a magical land, there was a {specific_choice} who\"\n",
    "    else:\n",
    "        print(\"That's not a valid choice. Please type a number from 1 to 15.\")\n",
    "        return\n",
    "\n",
    "    # Generate the beginning of the story\n",
    "    story_parts = [prompt]\n",
    "    story = generate_response(prompt, model, tokenizer)\n",
    "    print(story)\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\nWhat happens next?\")\n",
    "        print(\"Type the number to choose:\")\n",
    "        print(\"1. The character encounters a challenge.\")\n",
    "        print(\"2. The character makes a new friend.\")\n",
    "        print(\"3. The character discovers something amazing.\")\n",
    "        print(\"4. Summarize the story and finish.\")\n",
    "        user_input = input(\"> \").strip()\n",
    "\n",
    "        if user_input == \"1\":\n",
    "            prompt = f\"The {specific_choice} faced a great challenge. It was...\"\n",
    "            story_parts.append(prompt)\n",
    "        elif user_input == \"2\":\n",
    "            prompt = f\"The {specific_choice} made a new friend. This friend was...\"\n",
    "            story_parts.append(prompt)\n",
    "        elif user_input == \"3\":\n",
    "            prompt = f\"The {specific_choice} discovered something amazing. It was...\"\n",
    "            story_parts.append(prompt)\n",
    "        elif user_input == \"4\":\n",
    "            print(\"Summarizing the story and finishing it.\")\n",
    "            story_parts.append(story)\n",
    "            summary = \" \".join(story_parts)\n",
    "            print(f\"\\nHere is the summary of your story:\\n\\n{summary}\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid choice. Please type 1, 2, 3, or 4.\")\n",
    "            continue\n",
    "\n",
    "        # Generate the next part of the story\n",
    "        story = generate_response(prompt, model, tokenizer)\n",
    "        print(story)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    interactive_storytelling()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff899350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.43.3-py3-none-any.whl (9.4 MB)\n",
      "     ---------------------------------------- 9.4/9.4 MB 4.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\shyni\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shyni\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Collecting tokenizers<0.20,>=0.19\n",
      "  Downloading tokenizers-0.19.1-cp39-none-win_amd64.whl (2.2 MB)\n",
      "     ---------------------------------------- 2.2/2.2 MB 4.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shyni\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\shyni\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\shyni\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shyni\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.4.3-cp39-none-win_amd64.whl (287 kB)\n",
      "     -------------------------------------- 287.9/287.9 kB 6.0 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub<1.0,>=0.23.2\n",
      "  Using cached huggingface_hub-0.24.2-py3-none-any.whl (417 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shyni\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shyni\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\shyni\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\shyni\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shyni\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\shyni\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shyni\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\shyni\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Installing collected packages: safetensors, fsspec, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.7.1\n",
      "    Uninstalling fsspec-2022.7.1:\n",
      "      Successfully uninstalled fsspec-2022.7.1\n",
      "Successfully installed fsspec-2024.6.1 huggingface-hub-0.24.2 safetensors-0.4.3 tokenizers-0.19.1 transformers-4.43.3\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8463ad10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch\n",
      "  Using cached pytorch-1.0.2.tar.gz (689 bytes)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: pytorch\n",
      "  Building wheel for pytorch (setup.py): started\n",
      "  Building wheel for pytorch (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for pytorch\n",
      "Failed to build pytorch\n",
      "Installing collected packages: pytorch\n",
      "  Running setup.py install for pytorch: started\n",
      "  Running setup.py install for pytorch: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [6 lines of output]\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 2, in <module>\n",
      "    File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "    File \"C:\\Users\\SHYNI\\AppData\\Local\\Temp\\pip-install-iye9hyye\\pytorch_beb20baf11594b6d8c08d1a83ce82361\\setup.py\", line 15, in <module>\n",
      "      raise Exception(message)\n",
      "  Exception: You tried to install \"pytorch\". The package named for PyTorch is \"torch\"\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for pytorch\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Running setup.py install for pytorch did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [6 lines of output]\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 2, in <module>\n",
      "    File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "    File \"C:\\Users\\SHYNI\\AppData\\Local\\Temp\\pip-install-iye9hyye\\pytorch_beb20baf11594b6d8c08d1a83ce82361\\setup.py\", line 11, in <module>\n",
      "      raise Exception(message)\n",
      "  Exception: You tried to install \"pytorch\". The package named for PyTorch is \"torch\"\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: legacy-install-failure\n",
      "\n",
      "Encountered error while trying to install package.\n",
      "\n",
      "pytorch\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for output from the failure.\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e57164",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
